# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
# --------------------------------------------------------------------------


from collections import abc
from typing import Callable, List, Optional, Set, Union

import torch

from onnxruntime.training.utils.io import flatten_data_with_schema, unflatten_from_data_and_schema

from ._subscriber_base import SubscriberBase, _RuntimeStates


class _IncrementStep(torch.autograd.Function):
    """
    This class is used to manage the global execution step, e.g.
    global step increment by one, once a full forward path is completed and the state clear.

    This autograd Function is registered as a post-forward hook to the root module. So once the root
    module's forward path is completed, this backward function will be called immediately, triggering
    global step increment and state clear.
    """

    @staticmethod
    def forward(ctx, run_ctx: _RuntimeStates, input_tensor: torch.Tensor):
        """
        Make sure there is a same number of `tensor` inputs and outputs.
        This is enforced by ORT's PythonOp's schema check.
        """
        ctx.current_step = run_ctx.global_states.execution_step
        ctx.run_ctx = run_ctx

        # We cannot do the step incremental here. Imagine the outside-most module has multiple outputs,
        # we need to increase the step only at the very last output handling.
        # We avoid the complexity to probe the last output handling, and instead, we assume once
        # the very first backward of the outside-most module is called, then the forward pass MUST be completed.

        # Be noted: it is not safe to register _IncrementStep only for one of the outputs of the outside-most module,
        # because we are not sure which output branch is executed earlier, for example.
        #                                   OuterMostModuleOutputs
        #                                 /                         \
        #  OuterMostModuleOutputs_0_0th_output             OuterMostModuleOutputs_0_1th_output
        #                    |                                            |
        #         PythonOp(_InspectActivation)                  PythonOp(_InspectActivation)
        #                    |                                            |
        #          PythonOp(_IncrementStep)                           graph output
        #                    |
        #                graph output
        # The PythonOp(_InspectActivation) (who relies on global step) after 1th output is possible
        # to run before or after PythonOp(_IncrementStep), so increasing the step is not safe.

        return input_tensor.detach() if isinstance(input_tensor, torch.Tensor) else input_tensor

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor):
        # In case there are multiple backward calls for multiple outputs of the outside-most module.
        if ctx.current_step == ctx.run_ctx.global_states.execution_step:
            if ctx.current_step >= 0:
                print(f"{'='*6} Completed forward pass for STEP {ctx.current_step} {'='*6}")
            ctx.run_ctx.global_states.execution_step += 1
            ctx.run_ctx.reset_step_states()

        return None, grad_output.detach() if isinstance(grad_output, torch.Tensor) else grad_output


class SubscriberManager:
    """
    This class is used to manage all the subscribers and register the post-forward hook to the root module.
    `subscribe()` is used to register a list of subscribers.

    Currently, the hook handled here is post forward hook for nn.Module. The hook is registered for all nn.Modules
    recursively. Each hook inserts a PythonOp for every tensor output generated by the corresponding module.
    Each subscriber implementation is called in the PythonOp's forward function, and backward function.

    There is one special handling for global step increment and state clear. A post-forward hook is registered
    for the outside-most module, which is the root module. In that hook, _IncrementStep is called, which will
    increase the step by 1 once the very first time its backward is called (check _IncrementStep for details).
    """

    def __init__(self):
        self._run_ctx: _RuntimeStates = _RuntimeStates()
        self._subscribers: Set[SubscriberBase] = set()

    def subscribe(self, module: torch.nn.Module, subscribers: List[SubscriberBase]):
        """
        The API is called externally to register hooks that are implicitly defined by subscribers.
        Each time all global states will be cleaned up once called.
        """
        if not isinstance(module, torch.nn.Module):
            raise ValueError("module must be a torch.nn.Module instance")

        self._reset_all_states()
        self._subscribers.clear()

        try:
            from onnxruntime.training.ortmodule import ORTModule

            if isinstance(module, ORTModule):
                module = module.module
        except ImportError:
            pass

        for subscriber in subscribers:
            if not isinstance(subscriber, SubscriberBase):
                raise ValueError("subscriber must be a SubscriberBase instance")
            self._subscribers.add(subscriber)

        self._initialize(module)

    def get_run_context(self) -> _RuntimeStates:
        return self._run_ctx

    def _reset_all_states(self):
        self._run_ctx = _RuntimeStates()

    def _initialize(self, module: torch.nn.Module):
        """Register hooks for the specified module."""
        if len(self._subscribers) == 0:
            raise RuntimeError("No subscribers are registered.")

        def _pre_forward_outmost_module_hook(module, module_inputs):
            # This check is to support the case where module is first registered in the subscriber manager,
            # then the module and hook are copied, when new module instance runs to the hook, the global states
            # are not reset, so the logic depends on the global states will fail. So in the outer-most pre-forward hook
            # we reset the global states.

            # Be noted, the first run anyway will run in PyTorch.
            if module not in self._run_ctx.global_states.module_to_module_index:
                import warnings

                warnings.warn(
                    "reset global states due to module is not found in self._run_ctx.global_states.module_to_module_index"
                )
                self._reset_global_states(module)
            return module_inputs

        module.register_forward_pre_hook(_pre_forward_outmost_module_hook)

        # Register post forward hook for every module, inside the hook, we loop every tensor output of the module,
        # and wrap it with an torch.autograd.Function called _InspectActivation (which takes in a tensor and returns
        # the same tensor). In this way, we keep ORT and PyTorch run have the same boundary to check activation
        # equality.
        next_module_index = [0]
        self._register_hooks_recursively(module, 1, next_module_index)

        # Register post forward hook for the outside-most module, then we increase the dump step.
        # Be noted, if backward is not triggered, the global dump step remains the original number,
        # which means the subsequent run will override the previous dump files. This indeed happens to imagine ORTModule
        # firstly export graph (run the forward only), after the gradient graph is built, another forward+backward is
        # triggered, override the previous dump files.
        def _post_forward_outmost_module_hook(module, _, module_outputs):
            # def _apply_to_tensors_func(_, outputs):
            #     return _IncrementStep.apply(self._run_ctx, outputs)

            # return self._apply_function_to_tensors(module, module_outputs, _apply_to_tensors_func)

            output_schema, flatten_output_tensor_list = flatten_data_with_schema(module_outputs)
            for i, output_tensor in enumerate(flatten_output_tensor_list):
                output_tensor = _IncrementStep.apply(self._run_ctx, output_tensor)
                flatten_output_tensor_list[i] = output_tensor

            return unflatten_from_data_and_schema(flatten_output_tensor_list, output_schema)

        module.register_forward_hook(_post_forward_outmost_module_hook)

    def _reset_global_states(self, module: torch.nn.Module):
        def _reset_recursively(module: torch.nn.Module, depth: int, next_module_index: List[int]):
            """
            Called to register hooks for every `torch.nn.Module`. Due to `Module` can contain child `Module`s,
            this function is called recursively by passing in `next_module_index` - a list of int to maintain a
            global incremental unique module id.

            Args:
                module: torch.nn.Module to register hook.
                depth: the indent of the module compared with the outside-most Module.
                next_module_index: list of int, carrying a global unique module index that can be used next.
            """
            module_index = next_module_index[0]
            self._run_ctx.global_states.module_index_to_depth[module_index] = depth
            self._run_ctx.global_states.module_to_module_index[module] = module_index

            for child in module.children():
                if (
                    isinstance(child, torch.nn.Module)
                    and child not in self._run_ctx.global_states.module_to_module_index
                ):
                    next_module_index[0] += 1
                    _reset_recursively(child, depth + 1, next_module_index)

        next_module_index = [0]
        _reset_recursively(module, 1, next_module_index)

    def _register_hooks_recursively(self, module: torch.nn.Module, depth: int, next_module_index: List[int]):
        """
        Called to register hooks for every `torch.nn.Module`. Due to `Module` can contain child `Module`s,
        this function is called recursively by passing in `next_module_index` - a list of int to maintain a
        global incremental unique module id.

        Args:
            module: torch.nn.Module to register hook.
            depth: the indent of the module compared with the outside-most Module.
            next_module_index: list of int, carrying a global unique module index that can be used next.
        """
        module_index = next_module_index[0]
        self._run_ctx.global_states.module_index_to_depth[module_index] = depth
        self._run_ctx.global_states.module_to_module_index[module] = module_index

        for child in module.children():
            if isinstance(child, torch.nn.Module) and child not in self._run_ctx.global_states.module_to_module_index:
                next_module_index[0] += 1
                self._register_hooks_recursively(child, depth + 1, next_module_index)

        def _pre_forward_module_hook(module, module_inputs):
            input_schema, flatten_input_tensor_list = flatten_data_with_schema(module_inputs)

            # Module level hook
            for sub in self._subscribers:
                flatten_input_tensor_list = sub.pre_forward_module_apply(
                    module, self._run_ctx, flatten_input_tensor_list
                )

            # Tensor level hook
            for sub in self._subscribers:
                tensor_list = []
                for tensor_index, tensor in enumerate(flatten_input_tensor_list):
                    tensor_list.append(sub.pre_forward_tensor_apply(module, self._run_ctx, tensor_index, tensor))
                flatten_input_tensor_list = tensor_list

            return unflatten_from_data_and_schema(flatten_input_tensor_list, input_schema)

        def _post_forward_module_hook(module, module_inputs, module_outputs):
            _, flatten_input_tensor_list = flatten_data_with_schema(module_inputs)
            output_schema, flatten_output_tensor_list = flatten_data_with_schema(module_outputs)

            # Module level hook
            for sub in self._subscribers:
                _, flatten_output_tensor_list = sub.post_forward_module_apply(
                    module, self._run_ctx, flatten_input_tensor_list, flatten_output_tensor_list
                )

            # Tensor level hook
            for sub in self._subscribers:
                tensor_list = []
                for tensor_index, tensor in enumerate(flatten_output_tensor_list):
                    tensor_list.append(sub.post_forward_tensor_apply(module, self._run_ctx, tensor_index, tensor))
                flatten_output_tensor_list = tensor_list

            return unflatten_from_data_and_schema(flatten_output_tensor_list, output_schema)

        module.register_forward_pre_hook(_pre_forward_module_hook)
        module.register_forward_hook(_post_forward_module_hook)
