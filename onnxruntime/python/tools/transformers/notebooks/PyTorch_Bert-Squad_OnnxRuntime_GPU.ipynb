{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model with ONNX Runtime on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime and NVIDIA GPU. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "This notebook is for GPU inference. For CPU inference, please look at another notebook [Inference PyTorch Bert Model with ONNX Runtime on CPU](PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "It requires your machine to have a GPU, and a python environment with [PyTorch](https://pytorch.org/) installed before running this notebook.\n",
    "\n",
    "#### GPU Environment Setup using AnaConda\n",
    "\n",
    "First, we install [AnaConda](https://www.anaconda.com/distribution/) in a target machine and open an AnaConda prompt window when it is done. Then run the following commands to create a conda environment. This notebook is tested with PyTorch 2.0.1 and OnnxRuntime 1.15.1.\n",
    "\n",
    "```console\n",
    "conda create -n gpu_env python=3.10\n",
    "conda activate gpu_env\n",
    "pip install jupyterlab\n",
    "conda install ipykernel\n",
    "conda install -c conda-forge ipywidgets\n",
    "ipython kernel install --user --name gpu_env\n",
    "jupyter-lab\n",
    "```\n",
    "Finally, launch Jupyter Notebook and you can choose gpu_env as kernel to run this notebook.\n",
    "\n",
    "Onnxruntime-gpu need specified version of CUDA and cuDNN. You can find the Requirements [here](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements). Remember to add the directories to PATH environment variable (See [CUDA and cuDNN Path](#CUDA-and-cuDNN-Path) below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.platform in ['linux', 'win32']: # Linux or Windows\n",
    "    !{sys.executable} -m pip install torch --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "    !{sys.executable} -m pip install onnxruntime-gpu==1.15.1 onnx transformers==4.18 psutil pandas py-cpuinfo py3nvml coloredlogs wget netron sympy -q\n",
    "else: # Mac\n",
    "    print(\"CUDA is not available on MacOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA and cuDNN Path\n",
    "onnxruntime-gpu has dependency on [CUDA](https://developer.nvidia.com/cuda-downloads) and [cuDNN](https://developer.nvidia.com/cudnn). Required CUDA version can be found [here](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cuda_path = False\n",
    "\n",
    "# For Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#environment-setup\n",
    "# Below is example for Windows\n",
    "if add_cuda_path:\n",
    "    cuda_dir = 'H:\\\\CUDA_11.8\\\\bin'\n",
    "    cudnn_dir = 'H:\\\\cudnn-windows-x86_64-8.5.0.96_cuda11\\\\bin'\n",
    "    if not (os.path.exists(cuda_dir) and os.path.exists(cudnn_dir)):\n",
    "        raise ValueError(\"Please specify correct path for CUDA and cuDNN. Otherwise onnxruntime cannot be imported.\")\n",
    "    else:\n",
    "        if cuda_dir == cudnn_dir:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + os.environ[\"PATH\"]\n",
    "        else:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + cudnn_dir + ';' + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: 2.0.1+cu118\n",
      "onnxruntime: 1.15.1\n",
      "onnx: 1.14.0\n",
      "transformers: 4.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import transformers\n",
    "print(\"pytorch:\", torch.__version__)\n",
    "print(\"onnxruntime:\", onnxruntime.__version__)\n",
    "print(\"onnx:\", onnx.__version__)\n",
    "print(\"transformers:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the SQuAD data file and store them in the specified location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./squad\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some constant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether allow overwriting existing ONNX model and download the latest script from GitHub\n",
    "enable_overwrite = True\n",
    "\n",
    "# Total samples to inference, so that we can get average latency\n",
    "total_samples = 1000\n",
    "\n",
    "# ONNX opset version\n",
    "opset_version=11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some model configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuned model from https://huggingface.co/models?search=squad\n",
    "model_name_or_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load model from pretrained. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:04<00:00, 11.48it/s]\n",
      "convert squad examples to features: 100%|█████████████████████████████████████████| 1000/1000 [00:08<00:00, 118.00it/s]\n",
      "add example index and unique id: 100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 996982.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "\n",
    "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "# load some examples\n",
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:total_samples], # convert enough examples for this notebook\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model exported at  .\\onnx_models\\bert-base-cased-squad_opset11.onnx\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(\".\", \"onnx_models\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "export_model_path = os.path.join(output_dir, 'bert-base-cased-squad_opset{}.onnx'.format(opset_version))\n",
    "\n",
    "import torch\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "\n",
    "# Get the first example data to run the model and export it to ONNX\n",
    "data = dataset[0]\n",
    "inputs = {\n",
    "    'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "    'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "    'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "}\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if enable_overwrite or not os.path.exists(export_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
    "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=opset_version,                      # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask', \n",
    "                                       'segment_ids'],\n",
    "                          output_names=['start', 'end'],                    # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'segment_ids' : symbolic_names,\n",
    "                                        'start' : symbolic_names,\n",
    "                                        'end' : symbolic_names})\n",
    "        print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Inference ##\n",
    "Use PyTorch to evaluate an example input for comparison purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cuda Inference time = 21.42 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
    "latency = []\n",
    "with torch.no_grad():\n",
    "    for i in range(total_samples):\n",
    "        data = dataset[i]\n",
    "        inputs = {\n",
    "            'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "            'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "            'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "        }\n",
    "        start = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        latency.append(time.time() - start)\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference ONNX Model with ONNX Runtime ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference the model with ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnnxRuntime gpu Inference time = 15.82 ms\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
    "device_name = 'gpu'\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "# Note that this will increase session creation time so enable it for debugging only.\n",
    "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
    "\n",
    "# Please change the value according to best setting in Performance Test Tool result.\n",
    "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "latency = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    ort_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "    start = time.time()\n",
    "    ort_outputs = session.run(None, ort_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "    \n",
    "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the output of PyTorch and ONNX Runtime. We can see some results are not close. It is because ONNX Runtime uses some approximation in CUDA optimization. Based on our evaluation on SQuAD data set, F1 score is on par for models before and after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "maximum_diff=0.006359100341796875 average_diff=0.000678015872836113\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n",
      "maximum_diff=0.004163980484008789 average_diff=0.0005956395179964602\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):    \n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[i], outputs[i].cpu(), rtol=1e-02, atol=1e-02))\n",
    "    diff = ort_outputs[i] - outputs[i].cpu().numpy()\n",
    "    max_diff = numpy.max(numpy.abs(diff))\n",
    "    avg_diff = numpy.average(numpy.abs(diff))\n",
    "    print(f'maximum_diff={max_diff} average_diff={avg_diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Actual Sequence Length\n",
    "Note that ONNX model is exported using dynamic length axis. It is recommended to use actual sequence input without padding instead of fixed length input for best performance. Let's see how it can be applied to this model.\n",
    "\n",
    "From an example input below, we can see zero padding at the end of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2329,  2694,  2897,  2097,  4287,  1996,  3565,  4605,\n",
       "           1029,   102,  1999,  1996,  2142,  2983,  1010,  4035,  2557,  1019,\n",
       "           2444,  1998,  1019,  2444,  2998,  4469,  2097,  4287,  1996,  5049,\n",
       "           1012,  1996,  4035,  2097,  4287,  2049,  2219,  2329,  2394,  3743,\n",
       "           1010,  2007,  6754, 10184,  1010, 12270, 10589,  1998,  6857,  8945,\n",
       "          18505,  2006,  8570,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example input (we can see padding). From attention_mask, we can deduce the actual length.\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original sequence length is 128. After removing paddings, the sequence length is reduced. Input with smaller sequence length need less computation, thus we can see there is improvement on inference latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length 94\n",
      "OnnxRuntime gpu Inference time with actual sequence length = 13.63 ms\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "latency = []\n",
    "lengths = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    # Instead of using fixed length (128), we can use actual sequence length (less than 128), which helps to get better performance.\n",
    "    actual_sequence_length = sum(data[1].numpy())\n",
    "    lengths.append(actual_sequence_length)\n",
    "    opt_inputs = {\n",
    "        'input_ids':  data[0].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
    "        'input_mask': data[1].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
    "        'segment_ids': data[2].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length)\n",
    "    }\n",
    "    start = time.time()\n",
    "    opt_outputs = session.run(None, opt_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "print(\"Average length\", statistics.mean(lengths))\n",
    "print(\"OnnxRuntime {} Inference time with actual sequence length = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the output and see whether the results are close.\n",
    "\n",
    "**Note**: Need end-to-end evaluation on performance and accuracy if you use this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Comparing results with/without paddings *****\n",
      "Output 0 are close: False\n",
      "Output 1 are close: False\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Comparing results with/without paddings *****\")\n",
    "for i in range(2):\n",
    "    print('Output {} are close:'.format(i), numpy.allclose(opt_outputs[i], ort_outputs[i][:,:len(opt_outputs[i][0])], rtol=1e-03, atol=1e-03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Offline Optimization and Test Tools\n",
    "\n",
    "It is recommended to try [OnnxRuntime Transformer Model Optimization Tool](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers) on the exported ONNX models. It could help verify whether the model can be fully optimized, and get performance test results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Optimizer\n",
    "\n",
    "Although OnnxRuntime could optimize Bert model exported by PyTorch. Sometime, model cannot be fully optimized due to different reasons:\n",
    "* A new subgraph pattern is generated by new version of export tool, and the pattern is not covered by older version of OnnxRuntime. \n",
    "* The exported model uses dynamic axis and this makes it harder for shape inference of the graph. That blocks some optimization to be applied.\n",
    "* Some optimization is better to be done offline. Like change input tensor type from int64 to int32 to avoid extra Cast nodes, or convert model to float16 to achieve better performance in V100 or T4 GPU.\n",
    "\n",
    "We have python script **optimizer.py**, which is more flexible in graph pattern matching and model conversion (like float32 to float16). You can also use it to verify whether a Bert model is fully optimized.\n",
    "\n",
    "In this example, we can see that it introduces optimization that is not provided by onnxruntime: SkipLayerNormalization and bias fusion, which is not fused in OnnxRuntime due to shape inference as mentioned.\n",
    "\n",
    "It will also tell whether the model is fully optimized or not. If not, that means you might need change the script to fuse some new pattern of subgraph.\n",
    "\n",
    "Example Usage:\n",
    "```\n",
    "from onnxruntime.transformers import optimizer\n",
    "optimized_model = optimizer.optimize_model(export_model_path, model_type='bert', num_heads=12, hidden_size=768)\n",
    "optimized_model.save_model_to_file(optimized_model_path)\n",
    "```\n",
    "\n",
    "You can also use command line like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float32 Model\n",
    "Let us optimize the ONNX model using the script. The first example will output model with float32 to store weights. This is the choice for most GPUs without Tensor Core.\n",
    "\n",
    "If your GPU (like V100 or T4) has Tensor Core, jump to [Float16 Model](#6.-Model-Optimization-with-Float16) section since that will give you better performance than Float32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               apply: Fused LayerNormalization: 49\n",
      "               apply: Fused Gelu: 24\n",
      "               apply: Fused SkipLayerNormalization: 48\n",
      "               apply: Fused Attention: 24\n",
      "         prune_graph: Removed 5 nodes\n",
      "               apply: Fused EmbedLayerNormalization(with mask): 1\n",
      "         prune_graph: Removed 10 nodes\n",
      "               apply: Fused BiasGelu: 24\n",
      "               apply: Fused SkipLayerNormalization(add bias): 48\n",
      "            optimize: opset version: 11\n",
      "get_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 1, 'Attention': 24, 'MultiHeadAttention': 0, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 24, 'GemmFastGelu': 0, 'LayerNormalization': 0, 'SkipLayerNormalization': 48, 'QOrderedAttention': 0, 'QOrderedGelu': 0, 'QOrderedLayerNormalization': 0, 'QOrderedMatMul': 0}\n",
      "                main: The model has been fully optimized.\n",
      "  save_model_to_file: Sort graphs in topological order\n",
      "  save_model_to_file: Model saved to ./onnx/bert-base-cased-squad_opt_gpu_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "optimized_fp32_model_path = './onnx/bert-base-cased-squad_opt_{}_fp32.onnx'.format('gpu' if use_gpu else 'cpu')\n",
    "\n",
    "!{sys.executable} -m onnxruntime.transformers.optimizer --input $export_model_path --output $optimized_fp32_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Graph\n",
    "We can open the optimized model using [Netron](https://github.com/lutzroeder/netron) to visualize.\n",
    "\n",
    "The graph is like the following:\n",
    "<img src='images/optimized_bert_gpu.png'>\n",
    "\n",
    "Sometime, optimized graph is slightly different. For example, FastGelu is replaced by BiasGelu for CPU inference; When the option --input_int32 is used, Cast nodes for inputs are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "\n",
    "# change it to True if want to view the optimized model in browser\n",
    "enable_netron = False\n",
    "if enable_netron:\n",
    "    # If you encounter error \"access a socket in a way forbidden by its access permissions\", install Netron as standalone application instead.\n",
    "    netron.start(optimized_fp32_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Test Tool\n",
    "\n",
    "The following will create 1000 random inputs of batch_size 1 and sequence length 128, then measure the average latency and throughput numbers.\n",
    "\n",
    "Note that the test uses fixed sequence length. If you use [dynamic sequence length](#Inference-with-Actual-Sequence-Length), actual performance depends on the distribution of sequence length.\n",
    "\n",
    "**Attention**: Latency numbers from Jupyter Notebook are not accurate. See [Attional Info](#7.-Additional-Info) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.10 ms, Throughput = 82.66 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=11,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.22 ms, Throughput = 81.80 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=10,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.22 ms, Throughput = 81.83 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=9,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.36 ms, Throughput = 80.91 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=8,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.20 ms, Throughput = 81.95 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=7,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.29 ms, Throughput = 81.36 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=6,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.28 ms, Throughput = 81.41 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=5,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.27 ms, Throughput = 81.51 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=4,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.15 ms, Throughput = 82.31 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.19 ms, Throughput = 82.02 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=2,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.23 ms, Throughput = 81.77 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 12.25 ms, Throughput = 81.66 QPS\n",
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=None, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=1 sequence_length=128\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1_S128_20230806-235837.txt\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "\n",
    "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the summary file and take a look. Note that blank value in OMP_NUM_THREADS or OMP_WAIT_POLICY means the environment variable does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 model perf results from ./onnx\\perf_results_GPU_B1_S128_20230806-235837.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P50</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P95</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.10</td>\n",
       "      <td>12.07</td>\n",
       "      <td>12.11</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.24</td>\n",
       "      <td>12.52</td>\n",
       "      <td>82.66</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.15</td>\n",
       "      <td>12.13</td>\n",
       "      <td>12.19</td>\n",
       "      <td>12.26</td>\n",
       "      <td>12.41</td>\n",
       "      <td>12.81</td>\n",
       "      <td>82.31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.19</td>\n",
       "      <td>12.13</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.24</td>\n",
       "      <td>12.39</td>\n",
       "      <td>14.58</td>\n",
       "      <td>82.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.20</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.34</td>\n",
       "      <td>12.43</td>\n",
       "      <td>12.75</td>\n",
       "      <td>81.95</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.22</td>\n",
       "      <td>12.19</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.47</td>\n",
       "      <td>12.75</td>\n",
       "      <td>81.83</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.22</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.35</td>\n",
       "      <td>12.42</td>\n",
       "      <td>13.02</td>\n",
       "      <td>81.80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.23</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.22</td>\n",
       "      <td>12.30</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.87</td>\n",
       "      <td>81.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.25</td>\n",
       "      <td>12.21</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.34</td>\n",
       "      <td>12.45</td>\n",
       "      <td>13.05</td>\n",
       "      <td>81.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.27</td>\n",
       "      <td>12.22</td>\n",
       "      <td>12.28</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.52</td>\n",
       "      <td>12.98</td>\n",
       "      <td>81.51</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.28</td>\n",
       "      <td>12.26</td>\n",
       "      <td>12.30</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.49</td>\n",
       "      <td>13.03</td>\n",
       "      <td>81.41</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.29</td>\n",
       "      <td>12.26</td>\n",
       "      <td>12.31</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.51</td>\n",
       "      <td>13.05</td>\n",
       "      <td>81.36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.36</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>12.49</td>\n",
       "      <td>12.67</td>\n",
       "      <td>13.22</td>\n",
       "      <td>80.91</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Latency_P50  Latency_P75  Latency_P90  Latency_P95  \\\n",
       "0         12.10        12.07        12.11        12.17        12.24   \n",
       "1         12.15        12.13        12.19        12.26        12.41   \n",
       "2         12.19        12.13        12.18        12.24        12.39   \n",
       "3         12.20        12.17        12.25        12.34        12.43   \n",
       "4         12.22        12.19        12.25        12.36        12.47   \n",
       "5         12.22        12.18        12.25        12.35        12.42   \n",
       "6         12.23        12.18        12.22        12.30        12.40   \n",
       "7         12.25        12.21        12.25        12.34        12.45   \n",
       "8         12.27        12.22        12.28        12.40        12.52   \n",
       "9         12.28        12.26        12.30        12.36        12.49   \n",
       "10        12.29        12.26        12.31        12.40        12.51   \n",
       "11        12.36        12.32        12.39        12.49        12.67   \n",
       "\n",
       "    Latency_P99  Throughput(QPS)  intra_op_num_threads  \n",
       "0         12.52            82.66                    12  \n",
       "1         12.81            82.31                     4  \n",
       "2         14.58            82.02                     3  \n",
       "3         12.75            81.95                     8  \n",
       "4         12.75            81.83                    10  \n",
       "5         13.02            81.80                    11  \n",
       "6         12.87            81.77                     2  \n",
       "7         13.05            81.66                     1  \n",
       "8         12.98            81.51                     5  \n",
       "9         13.03            81.41                     6  \n",
       "10        13.05            81.36                     7  \n",
       "11        13.22            80.91                     9  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_GPU_B1_S128_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file)\n",
    "print(\"Float32 model perf results from\", latest_result_file)\n",
    "# Remove some columns that have same values for all rows.\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above result, we can see that latency is very close for different settings. The default setting (intra_op_num_threads=0, OMP_NUM_THREADS and OMP_WAIT_POLICY does not exist) performs the best. \n",
    "\n",
    "### Model Results Comparison Tool\n",
    "\n",
    "When a BERT model is optimized, some approximation is used in calculation. If your BERT model has three inputs, a script compare_bert_results.py can be used to do a quick verification. The tool will generate some fake input data, and compare the inference outputs of the original and optimized models. If outputs are all close, it is safe to use the optimized model.\n",
    "\n",
    "For GPU inference, the absolute or relative difference is larger than those numbers of CPU inference. Note that slight difference in output will not impact final result. We did end-to-end evaluation using SQuAD data set using a fine-tuned squad model, and F1 score is almost the same before/after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% passed for 100 random inputs given thresholds (rtol=0.01, atol=0.01).\n",
      "maximum absolute difference=0.020705461502075195\n",
      "maximum relative difference=0.0625513568520546\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.compare_bert_results --baseline_model $export_model_path --optimized_model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 100 --rtol 0.01 --atol 0.01 $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Optimization with Float16\n",
    "\n",
    "The optimizer.py script have an option **--float16** to convert model to use float16 to store weights. After the conversion, it could be faster to run in GPU with tensor cores like V100 or T4.\n",
    "\n",
    "Let's run tools to measure the performance on V100. The results show significant performance improvement: latency is about 3.4 ms for float32 model, and 1.8 ms for float16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed in shape inference <class 'MemoryError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               apply: Fused LayerNormalization: 49\n",
      "               apply: Fused Gelu: 24\n",
      "               apply: Fused SkipLayerNormalization: 48\n",
      "               apply: Fused Attention: 24\n",
      "         prune_graph: Removed 5 nodes\n",
      "               apply: Fused EmbedLayerNormalization(with mask): 1\n",
      "         prune_graph: Removed 10 nodes\n",
      "               apply: Fused BiasGelu: 24\n",
      "               apply: Fused SkipLayerNormalization(add bias): 48\n",
      "            optimize: opset version: 11\n",
      "convert_float_to_float16: \u001b[33mFailed to run symbolic shape inference. Please file an issue in https://github.com/microsoft/onnxruntime.\u001b[0m\n",
      "convert_np_to_float16: the float32 number 6.812558517310663e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -6.296501542379929e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.1344421341495945e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 7.30029281470479e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.1811716937870642e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 5.465552010974761e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.5459615809731986e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.2157979512039674e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 8.255485184349709e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.6365963634589207e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.1316992615538766e-10 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.3716379143602353e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 8.089911851527631e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.8019239433897383e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.5806841108533263e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.9390510530570282e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.523551311308438e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.52290266853106e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.3258444886996585e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.925671426292411e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.00972402555999e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.6805000058184305e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -9.700407588297821e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.547633488030442e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.692564641890385e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.3627577288843895e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.1047022852039845e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.528261226572795e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.6199043762508154e-10 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.1658107368361925e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.2127522925075027e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.2440270147351384e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.008938875836975e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 5.894755616964176e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.4322430292045283e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.9261777828869526e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.8675866126093297e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -9.028257252907679e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.957270079344653e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.2374369851595475e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.549040172856621e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.7115253093134015e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.859037178699509e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.9893738212317658e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.781269697356038e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.3500756068651754e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.374100998598806e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 6.836256893905102e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -8.753465507993496e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.3037486585053557e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.9365469228205257e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.5190219627168062e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.2541656602138573e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.897956097314136e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.193789600506534e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.733645653232088e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 7.160942949724358e-10 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.846586528131411e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.5771706425057346e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.7019313958144267e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.0584560722625156e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.3328196969459896e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.798548225437571e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.2394353866038728e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.4595643582637763e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.191208269332947e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.1799007104684733e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.4288541017037915e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.7786470962732892e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.014684596929328e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.1365335339519334e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.1147300860159248e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.683454569663013e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.5804585135347224e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -7.009989921868964e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.3333383286351363e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.6285825455451004e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.4461631770454915e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.2506468749506894e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.0044641562956258e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.6669562558367943e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.1117839799653666e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.0384265891616451e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.3765366058702284e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.0012151996363627e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 6.852389766720535e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -9.10312891733156e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.6715317957837215e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.508940859044742e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 6.498373839036731e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.6009333353727015e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 9.314499394008635e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.1930233689838587e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 6.665929142002369e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -7.183945882616172e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.7602879154310358e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.1566410051955245e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.233683625898266e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.9962876624267665e-10 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.476844228662458e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.065521670895464e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.2289574630462994e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.286249056737688e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.4261265763623214e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.752109651282808e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.9858703953777876e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.2689184570908765e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.0903405107428625e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.0608229284467825e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.0810330408617119e-10 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.380695723365079e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.1978403803332185e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.21821964632818e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.3627287742679073e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -6.973386312836283e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.265590020617992e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.2680286154420628e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.490708249704767e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 5.654450241365794e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.803507813733177e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 5.602208474897452e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.3799682224989738e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.3763440992752294e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.685718035801756e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.888910526939071e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.2003284588502083e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.7058029655458995e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.786988512719745e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.0136504847178e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.7629248040314e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 7.637570575980135e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -3.1512236375874636e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.102607142897341e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.194426151902462e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.2567365753900503e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.2618214735057336e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.654079870964779e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.6008542047529772e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.5042687923360063e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.195809921725413e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.5833972355494552e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.2634624013685425e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.863485886768103e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -5.088628185490052e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.6519145990278048e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.495700522966217e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.7497071286575192e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 5.110629786031495e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.2498613255473856e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.5943524661897754e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.0162829688908914e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.5216695459230323e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.900360840385474e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.5679951559377514e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -4.489617566605375e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.0496887803412847e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.0570434244859825e-10 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.240317252879322e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.772359858875916e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.8992494688063744e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 1.0873494105112513e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -2.2991754278223198e-08 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 4.588277313644085e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number 2.053232783794101e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.7088619408411887e-09 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 3.290076833195599e-08 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -6.142677477782854e-10 will be truncated to -5.96e-08\n",
      "convert_np_to_float16: the float32 number 6.543178443507713e-09 will be truncated to 5.96e-08\n",
      "convert_np_to_float16: the float32 number -1.3366164175465656e-08 will be truncated to -5.96e-08\n",
      "remove_useless_cast_nodes: Skip removing useless cast nodes since shape inference failed.\n",
      "get_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 1, 'Attention': 24, 'MultiHeadAttention': 0, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 24, 'GemmFastGelu': 0, 'LayerNormalization': 0, 'SkipLayerNormalization': 48, 'QOrderedAttention': 0, 'QOrderedGelu': 0, 'QOrderedLayerNormalization': 0, 'QOrderedMatMul': 0}\n",
      "                main: The model has been fully optimized.\n",
      "  save_model_to_file: Sort graphs in topological order\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnxruntime\\transformers\\optimizer.py\", line 491, in <module>\n",
      "    main()\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnxruntime\\transformers\\optimizer.py\", line 487, in main\n",
      "    optimizer.save_model_to_file(args.output, args.use_external_data_format)\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnxruntime\\transformers\\models\\gpt2\\..\\..\\onnx_model.py\", line 1052, in save_model_to_file\n",
      "    OnnxModel.save(self.model, output_path, use_external_data_format, all_tensors_to_one_file)\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnxruntime\\transformers\\models\\gpt2\\..\\..\\onnx_model.py\", line 1042, in save\n",
      "    save_model(model, output_path)\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnx\\__init__.py\", line 277, in save_model\n",
      "    serialized = _serialize(proto)\n",
      "  File \"f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnx\\__init__.py\", line 107, in _serialize\n",
      "    result = proto.SerializeToString()\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "optimized_fp16_model_path = './onnx/bert-base-cased-squad_opt_{}_fp16.onnx'.format('gpu' if use_gpu else 'cpu')\n",
    "!{sys.executable} -m onnxruntime.transformers.optimizer --input $export_model_path --output $optimized_fp16_model_path --float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "!python -m onnxruntime.transformers.bert_perf_test --model $optimized_fp16_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 $GPU_OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 model perf results from ./onnx\\perf_results_GPU_B1_S128_20230806-235837.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P50</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P95</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.10</td>\n",
       "      <td>12.07</td>\n",
       "      <td>12.11</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.24</td>\n",
       "      <td>12.52</td>\n",
       "      <td>82.66</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.15</td>\n",
       "      <td>12.13</td>\n",
       "      <td>12.19</td>\n",
       "      <td>12.26</td>\n",
       "      <td>12.41</td>\n",
       "      <td>12.81</td>\n",
       "      <td>82.31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.19</td>\n",
       "      <td>12.13</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.24</td>\n",
       "      <td>12.39</td>\n",
       "      <td>14.58</td>\n",
       "      <td>82.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.20</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.34</td>\n",
       "      <td>12.43</td>\n",
       "      <td>12.75</td>\n",
       "      <td>81.95</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.22</td>\n",
       "      <td>12.19</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.47</td>\n",
       "      <td>12.75</td>\n",
       "      <td>81.83</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.22</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.35</td>\n",
       "      <td>12.42</td>\n",
       "      <td>13.02</td>\n",
       "      <td>81.80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.23</td>\n",
       "      <td>12.18</td>\n",
       "      <td>12.22</td>\n",
       "      <td>12.30</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.87</td>\n",
       "      <td>81.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.25</td>\n",
       "      <td>12.21</td>\n",
       "      <td>12.25</td>\n",
       "      <td>12.34</td>\n",
       "      <td>12.45</td>\n",
       "      <td>13.05</td>\n",
       "      <td>81.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.27</td>\n",
       "      <td>12.22</td>\n",
       "      <td>12.28</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.52</td>\n",
       "      <td>12.98</td>\n",
       "      <td>81.51</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.28</td>\n",
       "      <td>12.26</td>\n",
       "      <td>12.30</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.49</td>\n",
       "      <td>13.03</td>\n",
       "      <td>81.41</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.29</td>\n",
       "      <td>12.26</td>\n",
       "      <td>12.31</td>\n",
       "      <td>12.40</td>\n",
       "      <td>12.51</td>\n",
       "      <td>13.05</td>\n",
       "      <td>81.36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.36</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>12.49</td>\n",
       "      <td>12.67</td>\n",
       "      <td>13.22</td>\n",
       "      <td>80.91</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Latency_P50  Latency_P75  Latency_P90  Latency_P95  \\\n",
       "0         12.10        12.07        12.11        12.17        12.24   \n",
       "1         12.15        12.13        12.19        12.26        12.41   \n",
       "2         12.19        12.13        12.18        12.24        12.39   \n",
       "3         12.20        12.17        12.25        12.34        12.43   \n",
       "4         12.22        12.19        12.25        12.36        12.47   \n",
       "5         12.22        12.18        12.25        12.35        12.42   \n",
       "6         12.23        12.18        12.22        12.30        12.40   \n",
       "7         12.25        12.21        12.25        12.34        12.45   \n",
       "8         12.27        12.22        12.28        12.40        12.52   \n",
       "9         12.28        12.26        12.30        12.36        12.49   \n",
       "10        12.29        12.26        12.31        12.40        12.51   \n",
       "11        12.36        12.32        12.39        12.49        12.67   \n",
       "\n",
       "    Latency_P99  Throughput(QPS)  intra_op_num_threads  \n",
       "0         12.52            82.66                    12  \n",
       "1         12.81            82.31                     4  \n",
       "2         14.58            82.02                     3  \n",
       "3         12.75            81.95                     8  \n",
       "4         12.75            81.83                    10  \n",
       "5         13.02            81.80                    11  \n",
       "6         12.87            81.77                     2  \n",
       "7         13.05            81.66                     1  \n",
       "8         12.98            81.51                     5  \n",
       "9         13.03            81.41                     6  \n",
       "10        13.05            81.36                     7  \n",
       "11        13.22            80.91                     9  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_GPU_B1_S128_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file)\n",
    "print(\"Float32 model perf results from\", latest_result_file)\n",
    "# Remove some columns that have same values for all rows.\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Tuning\n",
    "\n",
    "Some application need best throughput under some constraint on latency. This can be done by testing performance of different batch sizes. The tool could help on this.\n",
    "\n",
    "Here is an example that check the performance of multiple batch sizes (1, 2, 4, 8, 16, 32 and 64) using default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 5.93 ms, Throughput = 168.52 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=11,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 5.96 ms, Throughput = 167.76 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=10,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 5.95 ms, Throughput = 168.18 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=9,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 6.03 ms, Throughput = 165.94 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=8,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 6.00 ms, Throughput = 166.72 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=7,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 6.10 ms, Throughput = 163.94 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=6,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 34.97 ms, Throughput = 28.59 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=5,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 35.10 ms, Throughput = 28.49 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=4,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 10.62 ms, Throughput = 94.18 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 6.57 ms, Throughput = 152.16 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=2,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 6.19 ms, Throughput = 161.55 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 7.20 ms, Throughput = 138.79 QPS\n",
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=None, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=1 sequence_length=128\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1_S128_20230807-000911.txt\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=32,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 129.48 ms, Throughput = 247.15 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 6.72 ms, Throughput = 148.89 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=2,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 10.05 ms, Throughput = 198.98 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=4,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 18.27 ms, Throughput = 218.99 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=8,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 32.63 ms, Throughput = 245.16 QPS\n",
      "Running test: model=bert-base-cased-squad_opt_gpu_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=16,sequence_length=128,test_cases=1000,test_times=1,use_gpu=True\n",
      "Average latency = 58.82 ms, Throughput = 272.00 QPS\n",
      "test setting TestSetting(batch_size=32, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=3, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=32 sequence_length=128\n",
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=3, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=1 sequence_length=128\n",
      "test setting TestSetting(batch_size=2, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=3, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=2 sequence_length=128\n",
      "test setting TestSetting(batch_size=4, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=3, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=4 sequence_length=128\n",
      "test setting TestSetting(batch_size=8, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=3, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=8 sequence_length=128\n",
      "test setting TestSetting(batch_size=16, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=3, seed=3, verbose=False, log_severity=2)\n",
      "Generating 1000 samples for batch_size=16 sequence_length=128\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1-2-4-8-16-32_S128_20230807-001105.txt\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "THREAD_SETTING = '--intra_op_num_threads 3'\n",
    "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_fp16_model_path --batch_size 1 2 4 8 16 32 --sequence_length 128 --samples 1000 --test_times 1 $THREAD_SETTING $GPU_OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float16 model summary from ./onnx\\perf_results_GPU_B1-2-4-8-16-32_S128_20230807-001105.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P50</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P95</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.72</td>\n",
       "      <td>6.46</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.65</td>\n",
       "      <td>8.12</td>\n",
       "      <td>9.53</td>\n",
       "      <td>148.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.05</td>\n",
       "      <td>9.79</td>\n",
       "      <td>10.05</td>\n",
       "      <td>10.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>13.25</td>\n",
       "      <td>198.98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.27</td>\n",
       "      <td>16.96</td>\n",
       "      <td>17.49</td>\n",
       "      <td>19.46</td>\n",
       "      <td>30.05</td>\n",
       "      <td>33.69</td>\n",
       "      <td>218.99</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.63</td>\n",
       "      <td>32.55</td>\n",
       "      <td>32.68</td>\n",
       "      <td>32.88</td>\n",
       "      <td>33.10</td>\n",
       "      <td>34.58</td>\n",
       "      <td>245.16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.82</td>\n",
       "      <td>58.86</td>\n",
       "      <td>58.99</td>\n",
       "      <td>59.18</td>\n",
       "      <td>59.36</td>\n",
       "      <td>59.77</td>\n",
       "      <td>272.00</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>129.48</td>\n",
       "      <td>137.42</td>\n",
       "      <td>139.74</td>\n",
       "      <td>140.63</td>\n",
       "      <td>141.36</td>\n",
       "      <td>143.48</td>\n",
       "      <td>247.15</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latency(ms)  Latency_P50  Latency_P75  Latency_P90  Latency_P95  \\\n",
       "0         6.72         6.46         7.00         7.65         8.12   \n",
       "1        10.05         9.79        10.05        10.77        11.39   \n",
       "2        18.27        16.96        17.49        19.46        30.05   \n",
       "3        32.63        32.55        32.68        32.88        33.10   \n",
       "4        58.82        58.86        58.99        59.18        59.36   \n",
       "5       129.48       137.42       139.74       140.63       141.36   \n",
       "\n",
       "   Latency_P99  Throughput(QPS)  batch_size  \n",
       "0         9.53           148.89           1  \n",
       "1        13.25           198.98           2  \n",
       "2        33.69           218.99           4  \n",
       "3        34.58           245.16           8  \n",
       "4        59.77           272.00          16  \n",
       "5       143.48           247.15          32  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file)\n",
    "print(\"Float16 model summary from\", latest_result_file)\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'test_cases', 'test_times', 'use_gpu', 'sequence_length']\n",
    "columns_to_remove.extend(['intra_op_num_threads'])\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Info\n",
    "\n",
    "Note that running Jupyter Notebook has significant impact on performance result. You can close Jupyter Notebook and other applications, then run the performance test in a console to get more accurate performance numbers.\n",
    "\n",
    "We have a [benchmark script](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/run_benchmark.sh). It is recommended to use it measure inference speed of OnnxRuntime.\n",
    "\n",
    "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/main/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
    "\n",
    "Here is the machine configuration that generated the above results. You might get slower or faster result according to your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gpu\": {\n",
      "    \"driver_version\": \"472.88\",\n",
      "    \"devices\": [\n",
      "      {\n",
      "        \"memory_total\": 12884901888,\n",
      "        \"memory_available\": 6823723008,\n",
      "        \"name\": \"NVIDIA GeForce RTX 3060\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"cpu\": {\n",
      "    \"brand\": \"Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\",\n",
      "    \"cores\": 6,\n",
      "    \"logical_cores\": 12,\n",
      "    \"hz\": \"3192000000,0\",\n",
      "    \"l2_cache\": 1572864,\n",
      "    \"flags\": \"3dnow,3dnowprefetch,abm,acpi,adx,aes,apic,avx,avx2,bmi1,bmi2,clflush,clflushopt,cmov,cx16,cx8,de,dtes64,dts,erms,est,f16c,fma,fpu,fxsr,hle,ht,hypervisor,ia64,invpcid,lahf_lm,mca,mce,mmx,monitor,movbe,mpx,msr,mtrr,osxsave,pae,pat,pbe,pcid,pclmulqdq,pdcm,pge,pni,popcnt,pse,pse36,rdrnd,rdseed,rtm,sep,serial,sgx,sgx_lc,smap,smep,ss,sse,sse2,sse4_1,sse4_2,ssse3,tm,tm2,tsc,tscdeadline,vme,x2apic,xsave,xtpr\",\n",
      "    \"processor\": \"Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"total\": 16977195008,\n",
      "    \"available\": 7464935424\n",
      "  },\n",
      "  \"os\": \"Windows-10-10.0.22621-SP0\",\n",
      "  \"python\": \"3.10.12.final.0 (64 bit)\",\n",
      "  \"packages\": {\n",
      "    \"flatbuffers\": \"23.5.26\",\n",
      "    \"numpy\": \"1.25.2\",\n",
      "    \"onnx\": \"1.14.0\",\n",
      "    \"onnxruntime-gpu\": \"1.15.1\",\n",
      "    \"protobuf\": \"4.23.4\",\n",
      "    \"sympy\": \"1.11.1\",\n",
      "    \"torch\": \"2.0.1+cu118\",\n",
      "    \"transformers\": \"4.18.0\"\n",
      "  },\n",
      "  \"onnxruntime\": {\n",
      "    \"version\": \"1.15.1\",\n",
      "    \"support_gpu\": true\n",
      "  },\n",
      "  \"pytorch\": {\n",
      "    \"version\": \"2.0.1+cu118\",\n",
      "    \"support_gpu\": true,\n",
      "    \"cuda\": \"11.8\"\n",
      "  },\n",
      "  \"tensorflow\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\onnxruntime\\transformers\\machine_info.py:127: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m onnxruntime.transformers.machine_info --silent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
